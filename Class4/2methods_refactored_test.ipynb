{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/25 11:05:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.cores\", 8).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Method 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_process(path, file_name):\n",
    "    df = spark.read.json(path + file_name)\n",
    "    df = df.select('_source.*')\n",
    "\n",
    "    df = df.withColumn(\"Type\",\n",
    "                       when((col(\"AppName\") == 'CHANNEL') | (col(\"AppName\") == 'DSHD') | (col(\"AppName\") == 'KPLUS') | (col(\"AppName\") == 'KPlus'), \"Truyền Hình\")\n",
    "                       .when((col(\"AppName\") == 'VOD') | (col(\"AppName\") == 'FIMS_RES') | (col(\"AppName\") == 'BHD_RES') |\n",
    "                             (col(\"AppName\") == 'VOD_RES') | (col(\"AppName\") == 'FIMS') | (col(\"AppName\") == 'BHD') | (\n",
    "                                     col(\"AppName\") == 'DANET'), \"Phim Truyện\")\n",
    "                       .when((col(\"AppName\") == 'RELAX'), \"Giải Trí\")\n",
    "                       .when((col(\"AppName\") == 'CHILD'), \"Thiếu Nhi\")\n",
    "                       .when((col(\"AppName\") == 'SPORT'), \"Thể Thao\")\n",
    "                       .otherwise(\"Error\"))\n",
    "\n",
    "    df = df.select('Contract', 'Type', 'TotalDuration')\n",
    "    df = df.filter(df.Type != 'Error')\n",
    "\n",
    "    result = df.groupBy('Contract', 'Type').sum()\n",
    "    result = result.withColumnRenamed('sum(TotalDuration)', 'TotalDuration')\n",
    "    result = df.groupBy(\"Contract\").pivot(\"Type\").sum(\"TotalDuration\")\n",
    "\n",
    "    print('Finished Processing {}'.format(file_name))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Processing 20220402.json\n",
      "Finished Processing 20220401.json\n",
      "-----------Saving Data ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 91:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Saved Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main_task(start_date_str, end_date_str):\n",
    "    path = \"/Users/habaokhanh/Study_BigData_Dataset/log_content/\"\n",
    "    list_file = [file for file in os.listdir(path) if file != '.DS_Store']\n",
    "    \n",
    "    result = None\n",
    "    for file_name in list_file:\n",
    "        date_str = file_name.split('_')[-1].split('.')[0]\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "        \n",
    "        start_date = datetime.strptime(start_date_str, \"%Y%m%d\").date()\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y%m%d\").date()\n",
    "\n",
    "        if start_date <= file_date <= end_date:\n",
    "            df = etl_process(path, file_name)\n",
    "            if result is None:\n",
    "                result = df\n",
    "            else:\n",
    "                result = result.union(df)\n",
    "\n",
    "    result = result.groupby('Contract').sum()\n",
    "    result = result.withColumnRenamed('sum(Giải Trí)', 'RelaxDuration') \\\n",
    "        .withColumnRenamed('sum(Phim Truyện)', 'MovieDuration') \\\n",
    "        .withColumnRenamed('sum(Thiếu Nhi)', 'ChildDuration') \\\n",
    "        .withColumnRenamed('sum(Thể Thao)', 'SportDuration') \\\n",
    "        .withColumnRenamed('sum(Truyền Hình)', 'TVDuration')\n",
    "\n",
    "    print('-----------Saving Data ---------')\n",
    "    result.repartition(1).write.csv('/Users/habaokhanh/Study_BigData_Dataset/log_content/clean/df_clean2', header=True)\n",
    "    \n",
    "    return print('Data Saved Successfully')\n",
    "\n",
    "\n",
    "main_task('20220401', '20220402')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "| Contract|RelaxDuration|MovieDuration|ChildDuration|SportDuration|TVDuration|\n",
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "|HNH644322|         null|         null|         null|         null|     19692|\n",
      "|BED008119|         null|         null|         null|         null|      7220|\n",
      "|BNFD36683|         null|         null|          315|         null|     15038|\n",
      "|CBFD04764|         null|         null|         null|         null|    167551|\n",
      "|GLD024214|         null|          165|         null|         null|     35938|\n",
      "|HNH795510|         null|        18713|          265|         null|    100481|\n",
      "|NBFD10014|         null|         null|         null|         null|    115857|\n",
      "|LDD028485|         null|         null|         null|         null|       328|\n",
      "|DNFD35509|         null|          872|         null|         null|     85096|\n",
      "|HND603711|         null|         7319|         null|         null|     40041|\n",
      "|CMD019779|           28|         null|         9947|         null|     72750|\n",
      "|HND517437|         null|         null|         3299|         null|     21801|\n",
      "|LDFD06234|         null|         null|         null|         null|     43800|\n",
      "|SGH865388|         null|         null|         null|         null|      5177|\n",
      "|SGH460474|         null|         null|         null|         null|      9098|\n",
      "|HNH582367|         null|         null|         null|         null|     29103|\n",
      "|HND535299|         null|         null|         null|         null|     20454|\n",
      "|QAFD14262|         null|         null|         null|         null|     86400|\n",
      "|TND026221|         null|        39438|         null|         null|     10605|\n",
      "|VTFD21947|         null|         null|         null|         null|     58489|\n",
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test=spark.read.csv('/Users/habaokhanh/Study_BigData_Dataset/log_content/clean/df_clean2', header=True)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Method 1</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_process(path, file_name):\n",
    "    spark = SparkSession.builder.config(\"spark.driver.memory\", \"8g\").config(\"spark.executor.cores\",8).getOrCreate()\n",
    "    \n",
    "    df = spark.read.json(path+file_name)\n",
    "    df = df.select('_source.*')\n",
    "    \n",
    "    df = df.withColumn(\"Type\",\n",
    "           when((col(\"AppName\") == 'CHANNEL') | (col(\"AppName\") =='DSHD')| (col(\"AppName\") =='KPLUS')| (col(\"AppName\") =='KPlus'), \"Truyền Hình\")\n",
    "        .when((col(\"AppName\") == 'VOD') | (col(\"AppName\") =='FIMS_RES')| (col(\"AppName\") =='BHD_RES')| \n",
    "              (col(\"AppName\") =='VOD_RES')| (col(\"AppName\") =='FIMS')| (col(\"AppName\") =='BHD')| (col(\"AppName\") =='DANET'), \"Phim Truyện\")\n",
    "        .when((col(\"AppName\") == 'RELAX'), \"Giải Trí\")\n",
    "        .when((col(\"AppName\") == 'CHILD'), \"Thiếu Nhi\")\n",
    "        .when((col(\"AppName\") == 'SPORT'), \"Thể Thao\")\n",
    "        .otherwise(\"Error\"))\n",
    "    \n",
    "    df = df.select('Contract','Type','TotalDuration')\n",
    "    df = df.filter(df.Type != 'Error')\n",
    "    \n",
    "    print('Finished Processing {}'.format(file_name))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Processing 20220402.json\n",
      "Finished Processing 20220401.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Saving Data ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 146:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 7.16612 to process the data\n",
      "Data Saved Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def main_task(start_date_str, end_date_str):\n",
    "    start_time = datetime.now()\n",
    "    path = '/Users/habaokhanh/Study_BigData_Dataset/log_content/'\n",
    "    list_file = [file for file in os.listdir(path) if file != '.DS_Store']\n",
    "            \n",
    "    result = None\n",
    "    for file_name in list_file:\n",
    "        date_str = file_name.split('_')[-1].split('.')[0]\n",
    "        file_date = datetime.strptime(date_str, \"%Y%m%d\").date()\n",
    "        \n",
    "        start_date = datetime.strptime(start_date_str, \"%Y%m%d\").date()\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y%m%d\").date()\n",
    "\n",
    "        if start_date <= file_date <= end_date:\n",
    "            df = etl_process(path, file_name)\n",
    "            if result is None:\n",
    "                result = df\n",
    "            else:\n",
    "                result = result.union(df)\n",
    "    \n",
    "    result = result.groupby('Contract','Type').sum()\n",
    "    result = result.withColumnRenamed('sum(TotalDuration)','TotalDuration')\n",
    "    final = result.groupBy(\"Contract\").pivot(\"Type\").sum(\"TotalDuration\")\n",
    "\n",
    "    final = final.withColumnRenamed('Giải Trí', 'RelaxDuration') \\\n",
    "        .withColumnRenamed('Phim Truyện', 'MovieDuration') \\\n",
    "        .withColumnRenamed('Thiếu Nhi', 'ChildDuration') \\\n",
    "        .withColumnRenamed('Thể Thao', 'SportDuration') \\\n",
    "        .withColumnRenamed('Truyền Hình', 'TVDuration')\n",
    "    \n",
    "    print('-----------Saving Data ---------')\n",
    "    final.repartition(1).write.csv('/Users/habaokhanh/Study_BigData_Dataset/log_content/clean/df_clean1',header=True)\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    time_processing = (end_time - start_time).total_seconds()\n",
    "    print('It took {} to process the data'.format(time_processing))\n",
    "    \n",
    "    return print('Data Saved Successfully')\n",
    "\n",
    "main_task('20220401', '20220402')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "| Contract|RelaxDuration|MovieDuration|ChildDuration|SportDuration|TVDuration|\n",
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "|HNH644322|         null|         null|         null|         null|     19692|\n",
      "|BED008119|         null|         null|         null|         null|      7220|\n",
      "|BNFD36683|         null|         null|          315|         null|     15038|\n",
      "|CBFD04764|         null|         null|         null|         null|    167551|\n",
      "|GLD024214|         null|          165|         null|         null|     35938|\n",
      "|HNH795510|         null|        18713|          265|         null|    100481|\n",
      "|NBFD10014|         null|         null|         null|         null|    115857|\n",
      "|LDD028485|         null|         null|         null|         null|       328|\n",
      "|DNFD35509|         null|          872|         null|         null|     85096|\n",
      "|HND603711|         null|         7319|         null|         null|     40041|\n",
      "|CMD019779|           28|         null|         9947|         null|     72750|\n",
      "|HND517437|         null|         null|         3299|         null|     21801|\n",
      "|LDFD06234|         null|         null|         null|         null|     43800|\n",
      "|SGH865388|         null|         null|         null|         null|      5177|\n",
      "|SGH460474|         null|         null|         null|         null|      9098|\n",
      "|HNH582367|         null|         null|         null|         null|     29103|\n",
      "|HND535299|         null|         null|         null|         null|     20454|\n",
      "|QAFD14262|         null|         null|         null|         null|     86400|\n",
      "|TND026221|         null|        39438|         null|         null|     10605|\n",
      "|VTFD21947|         null|         null|         null|         null|     58489|\n",
      "+---------+-------------+-------------+-------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test=spark.read.csv('/Users/habaokhanh/Study_BigData_Dataset/log_content/clean/df_clean1', header=True)\n",
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
